{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.data.dataset import TTSDataset\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nemo.collections.tts.models.base import SpectrogramGenerator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./reinstall.sh dev\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget text-unidecode scipy==1.7.3\n",
    "# !pip install phonemizer && apt-get update\n",
    "# apt-get install espeak-ng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tts/configs.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastPitch\n",
    "\n",
    "FastPitch is non-autoregressive model for mel-spectrogram generation based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference [paper](https://ieeexplore.ieee.org/abstract/document/9413889). \n",
    "\n",
    "### HiFiGAN\n",
    "\n",
    "HiFiGAN is a generative adversarial network (GAN) model that generates audio from mel spectrograms. The generator uses transposed convolutions to upsample mel spectrograms to audio [paper](https://arxiv.org/abs/2010.05646). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "* Creating manifests\n",
    "* Normalizing text\n",
    "* Phonemization\n",
    "* Creating supplementary data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating manifests \n",
    "\n",
    "I created the script `my_get_data.py` which reads the file `the_fu_mattia_pascal/metadata.csv` provided with the dataset and generates the following fields for each datapoint:\n",
    "1. `audio_filepath`: location of the wav file\n",
    "2. `duration`: duration of the wav file\n",
    "3. `text`: original text\n",
    "    \n",
    "After that, the script randomly splits the data into 3 buckets, `train_manifest.json`, `val_manifest.json` and `test_manifest.json`.\n",
    "\n",
    "Also `my_get_data_multi_speaker.py` works the same way, but for multiple datasets (generates multi speaker)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% datapoints go to validation set, 20% go to test set and the remaining 70% go to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python my_get_data.py \\\n",
    "    --data-root /home/giacomo/ \\\n",
    "    --val-size 0.1 \\\n",
    "    --test-size 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing text\n",
    "\n",
    "The script above, `get_data.py`, also generates another field per each datapoint:\n",
    "- `normalized_text`: normalized text via custom NeMo's text normalizer for Italian language:\n",
    "    ```\n",
    "    nemo_text_processing.text_normalization.normalize.Normalizer(lang=\"it\", input_case=\"cased\", overwrite_cache=True, cache_dir=str(file_path / \"cache_dir\"))\n",
    "    ```\n",
    "    [github nemo IT](https://github.com/NVIDIA/NeMo-text-processing/tree/main/nemo_text_processing/text_normalization/it)\n",
    "    \n",
    "Here are some example records:\n",
    "```json\n",
    "\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python my_phonemizer.py \\\n",
    "    --manifests /home/giacomo/il_fu_mattia_pascal/train_manifest.json /home/giacomo/il_fu_mattia_pascal/test_manifest.json /home/giacomo/il_fu_mattia_pascal/val_manifest.json \\\n",
    "    --language it \\\n",
    "    --preserve-punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the phonemize method, refer to the docs [here](https://github.com/bootphon/phonemizer/blob/master/phonemizer/backend/base.py#L137).\n",
    "\n",
    " `my_phonemizer.py` generates `train_manifest_phonemes.json`, `test_manifest_phonemes.json` and `val_manifest_phonemes.json` respectively.\n",
    "\n",
    "We are effectively doubling the size of our dataset. Each original record maps on to two records, one with original `normalized_text` field value and `is_phoneme` set to 0 and another with phonemized text and `is_phoneme` flag set to 1.\n",
    "\n",
    "Example:\n",
    "```json\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"is_phoneme\": 0}\n",
    "\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 n\\u0254! ora! \\u2014 ribat\\u02d0e kwe\\u028e\\u026a, affer\\u027eandole \\u028an brat\\u0283\\u02d0o e at\\u02d0irandola a se.\", \"is_phoneme\": 1}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating supplementary data\n",
    "\n",
    "To accelerate and stabilize our training, we also need to extract pitch for every audio, estimate pitch statistics (mean and std) and pre-calculate alignment prior matrices for alignment framework. To do this, all we need to do is iterate over our data one time.\n",
    "\n",
    "In the below method the arguments are as follows:\n",
    "- `sup_data_path` — path to the folder which contains supplementary data. If the supplementary data or the folder does not already exists then it will be created.\n",
    "\n",
    "- `sup_data_types` — types of supplementary data to be provided to the model.\n",
    "\n",
    "- `text_tokenizer` — text tokenizer object that we already created.\n",
    "\n",
    "- `text_normalizer` — text normalizer object that we already created.\n",
    "\n",
    "- `text_normalizer_call_kwargs` — dictionary of arguments to be used in calling the text normalizer that we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "# Text normalizer\n",
    "text_normalizer = Normalizer(\n",
    "    lang=\"it\", \n",
    "    input_case=\"cased\", \n",
    "    whitelist=\"/home/giacomo/NeMo-text-processing/nemo_text_processing/text_normalization/it/data/whitelist.tsv\"\n",
    ")\n",
    "\n",
    "text_normalizer_call_kwargs = {\n",
    "    \"punct_pre_process\": True,\n",
    "    \"punct_post_process\": True\n",
    "}\n",
    "\n",
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianPhonemesTokenizer\n",
    "# Text tokenizer\n",
    "text_tokenizer = ItalianPhonemesTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "normalizer = Normalizer(input_case='cased', lang='it')\n",
    "written = \"2 km/m dip. Fisica\"\n",
    "norm_it = normalizer.normalize(written, punct_post_process=True, verbose=True)\n",
    "print(norm_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianCharsTokenizer\n",
    "tokenizer = ItalianCharsTokenizer()\n",
    "text = \"Ma poiché esso rimaneva lì\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianPhonemesTokenizer\n",
    "tokenizer = ItalianPhonemesTokenizer()\n",
    "text = \"Ma poiché esso rimaneva lì\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calculate_supplementary_data(sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs):\n",
    "    # init train and val dataloaders\n",
    "    stages = [\"train\", \"val\"]\n",
    "    stage2dl = {}\n",
    "    for stage in stages:\n",
    "        ds = TTSDataset(\n",
    "            manifest_filepath=f\"/home/giacomo/il_fu_mattia_pascal/{stage}_manifest_phonemes.json\",\n",
    "            sample_rate=16000,\n",
    "            sup_data_path=sup_data_path,\n",
    "            sup_data_types=sup_data_types,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            window=\"hann\",\n",
    "            n_mels=80,\n",
    "            lowfreq=0,\n",
    "            highfreq=8000,\n",
    "            text_tokenizer=text_tokenizer,\n",
    "            text_normalizer=text_normalizer,\n",
    "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
    "\n",
    "        ) \n",
    "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=1, collate_fn=ds._collate_fn, num_workers=1)\n",
    "\n",
    "    # iteration over dataloaders\n",
    "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
    "    for stage, dl in stage2dl.items():\n",
    "        pitch_list = []\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
    "            pitch = pitches.squeeze(0)\n",
    "            pitch_list.append(pitch[pitch != 0])\n",
    "\n",
    "        if stage == \"train\":\n",
    "            pitch_tensor = torch.cat(pitch_list)\n",
    "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
    "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
    "            \n",
    "    return pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script should gives the following result:\n",
    "1. Creates two folders under `fastpitch_sup_data_folder` - `pitch` and `align_prior_matrix`\n",
    "2. Prints out the values for pitch_mean, pitch_std, pitch_min, pitch_max. Use these values while training FastPitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
    "\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
    "    fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pitch_mean, pitch_std, pitch_min, pitch_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pitch_max: 651.6829223632812\n",
    "* pitch_min: 65.4063949584961\n",
    "* pitch_mean: 159.78488159179688\n",
    "* pitch_std: 31.194143295288086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_max = 651.6829223632812\n",
    "pitch_min = 65.4063949584961\n",
    "pitch_mean = 159.78488159179688\n",
    "pitch_std = 31.194143295288086\n",
    "\n",
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this also via `extract_sup_data.py` script."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, the script results in something similar, where all default parameters are set in fastpitch_align.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(CUDA_VISIBLE_DEVICES=0  HYDRA_FULL_ERROR=1 python fastpitch.py --config-path . --config-name=fastpitch_align_ITA.yaml \\\n",
    "  sample_rate=16000 \\\n",
    "  train_dataset=/home/giacomo/il_fu_mattia_pascal/train_manifest_phonemes.json \\\n",
    "  validation_datasets=/home/giacomo/il_fu_mattia_pascal/val_manifest_phonemes.json \\\n",
    "  sup_data_path=fastpitch_sup_data_folder \\\n",
    "  exp_manager.exp_dir=resultITA_TTS \\\n",
    "  trainer.check_val_every_n_epoch=1 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. We use `CUDA_VISIBLE_DEVICES=0` to limit training to single GPU.\n",
    "2. For debugging you may also add the following flags: `HYDRA_FULL_ERROR=1`, `CUDA_LAUNCH_BLOCKING=1`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating FastPitch + pretrained HiFi-GAN\n",
    "\n",
    "Let's evaluate the quality of the FastPitch model generated so far using a HiFi-GAN model pre-trained on English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from nemo.collections.tts.models import HifiGanModel, FastPitchModel\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"E non le pare che fosse rosso, ad esempio, il lanternone della Virt\\u00f9 pagana?\" # text input to the model\n",
    "test_id = \"mattiapascal_12_pirandello3_f000058\" # identifier for the audio corresponding to the test text\n",
    "data_path = \"/home/giacomo/il_fu_mattia_pascal/wavs/\" # path to dataset folder with wav files from original dataset\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spec_fastpitch_ckpt(spec_gen_model, v_model, test):\n",
    "    with torch.no_grad():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        parsed = spec_gen_model.parse(str_input=test, normalize=True)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed)\n",
    "        print(spectrogram.size())\n",
    "        audio = v_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "    spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "    audio = audio.to('cpu').numpy()[0]\n",
    "    audio = audio / np.abs(audio).max()\n",
    "    return audio, spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hifigan models\n",
    "hfg_ngc = \"tts_en_lj_hifigan_ft_mixerttsx\" # NGC pretrained model name: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_lj_hifigan \n",
    "vocoder_model = HifiGanModel.from_pretrained(hfg_ngc, strict=False).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fastpitch\n",
    "import glob, os\n",
    "fastpitch_model_path = sorted(\n",
    "    glob.glob(\"/home/giacomo/NeMo/resultITA_TTS/FastPitch/2023-10-01_20-04-15/checkpoints/FastPitch--val_loss=nan-epoch=2-last.ckpt\"), \n",
    "    key=os.path.getmtime)[-1] # path_to_fastpitch_nemo_or_ckpt\n",
    "\n",
    "if \".nemo\" in fastpitch_model_path:\n",
    "    spec_gen_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()\n",
    "else:\n",
    "    spec_gen_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model, test)\n",
    "\n",
    "# visualize the spectrogram\n",
    "if spectrogram is not None:\n",
    "    imshow(spectrogram, origin=\"lower\")\n",
    "    plt.show()\n",
    "\n",
    "# audio\n",
    "print(\"original audio\")\n",
    "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=16000))\n",
    "print(\"predicted audio\")\n",
    "ipd.display(ipd.Audio(audio, rate=16000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning HiFi-GAN\n",
    "\n",
    "Improving speech quality by Finetuning HiFi-GAN on synthesized mel-spectrograms from FastPitch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_text = \"E non le pare che fosse rosso, ad esempio, il lanternone della Virt\\u00f9 pagana?\"\n",
    "test_audio_filepath = \"/home/giacomo/il_fu_mattia_pascal/wavs/mattiapascal_12_pirandello3_f000058.wav\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "\n",
    "def load_wav(audio_file):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "    return samples.transpose()\n",
    "\n",
    "def plot_logspec(spec, axis=None):    \n",
    "    librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(spec, ref=np.max),\n",
    "        y_axis='linear', \n",
    "        x_axis=\"time\",\n",
    "        fmin=0, \n",
    "        fmax=8000,\n",
    "        ax=axis\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original mel spectrogram generated from original audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading original melspec\")\n",
    "y, sr = librosa.load(test_audio_filepath)\n",
    "# change n_fft, win_length, hop_length parameters below based on your specific config file\n",
    "spectrogram2 = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, win_length=1024, hop_length=256)\n",
    "spectrogram = spectrogram2[ :80, :]\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plot_logspec(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel spectrogram predicted from FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading fastpitch melspec via generate_spectrogram\")\n",
    "with torch.no_grad():\n",
    "    text = spec_model.parse(test_audio_text, normalize=False)\n",
    "    spectrogram = spec_model.generate_spectrogram(\n",
    "      tokens=text, \n",
    "      speaker=None,\n",
    "    )\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "plot_logspec(spectrogram)\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above predicted spectrogram has the duration lower in frames which is not equal to the ground truth 498 frames. In order to finetune HiFi-GAN we need mel spectrogram predicted from FastPitch with ground truth alignment and duration.\n",
    "\n",
    "### Mel spectrogram predicted from FastPitch with groundtruth alignment and duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading fastpitch melspec via forward method with groundtruth alignment and duration\")\n",
    "with torch.no_grad():\n",
    "    device = spec_model.device\n",
    "    beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "    text = spec_model.parse(test_audio_text, normalize=False)\n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    audio = load_wav(test_audio_filepath)\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "    attn_prior = torch.from_numpy(\n",
    "      beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "    ).unsqueeze(0).to(text.device)\n",
    "    spectrogram = spec_model.forward(\n",
    "      text=text, \n",
    "      input_lens=text_len, \n",
    "      spec=spect, \n",
    "      mel_lens=spect_len, \n",
    "      attn_prior=attn_prior,\n",
    "      speaker=None,\n",
    "    )[0]\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plot_logspec(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finetuning without groundtruth alignment and duration has artifacts from the original audio (noise) that get passed on as input to the vocoder resulting in artifacts in vocoder output in the form of noise.\n",
    "- <b> On the other hand, `Mel spectrogram predicted from FastPitch with groundtruth alignment and duration` gives the best results because it enables HiFi-GAN to learn mel spectrograms generated by FastPitch as well as duration distributions closer to the real world (i.e. ground truth) durations. </b>\n",
    "\n",
    "From implementation perspective - we follow the same process described in [Finetuning FastPitch for a new speaker](FastPitch_Finetuning.ipynb) - i.e. take the latest checkpoint from FastPitch training and predict spectrograms for each of the input records in `train_manifest_text_normed.json`, `test_manifest_text_normed.json` and `val_manifest_text_normed.json`. NeMo provides an efficient script, [scripts/dataset_processing/tts/generate_mels.py](https://raw.githubusercontent.com/nvidia/NeMo/main/scripts/dataset_processing/tts/generate_mels.py), to generate Mel-spectrograms in the directory `NeMoGermanTTS/mels` and also create new JSON manifests with a suffix `_mel` by adding a new key `\"mel_filepath\"`. For example, `train_manifest_text_normed.json` corresponds to `train_manifest_text_normed_mel.json` saved in the same directory. You can run the following CLI to obtain the new JSON manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_mels.py \\\n",
    "    --cpu \\\n",
    "    --input-json-manifests /home/giacomo/il_fu_mattia_pascal/train_manifest.json /home/giacomo/il_fu_mattia_pascal/test_manifest.json /home/giacomo/il_fu_mattia_pascal/val_manifest.json \\\n",
    "    --fastpitch-model-ckpt {fastpitch_model_path} \\\n",
    "    --output-json-manifest-root ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
