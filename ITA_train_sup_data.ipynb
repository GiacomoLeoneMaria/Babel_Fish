{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.data.dataset import TTSDataset\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nemo.collections.tts.models.base import SpectrogramGenerator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./reinstall.sh dev\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget text-unidecode scipy==1.7.3\n",
    "# !pip install phonemizer && apt-get update\n",
    "# apt-get install espeak-ng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tts/configs.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastPitch\n",
    "\n",
    "FastPitch is non-autoregressive model for mel-spectrogram generation based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference [paper](https://ieeexplore.ieee.org/abstract/document/9413889). \n",
    "\n",
    "### HiFiGAN\n",
    "\n",
    "HiFiGAN is a generative adversarial network (GAN) model that generates audio from mel spectrograms. The generator uses transposed convolutions to upsample mel spectrograms to audio [paper](https://arxiv.org/abs/2010.05646). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "* Creating manifests\n",
    "* Normalizing text\n",
    "* Phonemization\n",
    "* Creating supplementary data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating manifests \n",
    "\n",
    "I created the script `my_get_data.py` which reads the file `the_fu_mattia_pascal/metadata.csv` provided with the dataset and generates the following fields for each datapoint:\n",
    "1. `audio_filepath`: location of the wav file\n",
    "2. `duration`: duration of the wav file\n",
    "3. `text`: original text\n",
    "    \n",
    "After that, the script randomly splits the data into 3 buckets, `train_manifest.json`, `val_manifest.json` and `test_manifest.json`.\n",
    "\n",
    "Also `my_get_data_multi_speaker.py` works the same way, but for multiple datasets (generates multi speaker)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% datapoints go to validation set, 20% go to test set and the remaining 70% go to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python my_get_data.py \\\n",
    "    --data-root /home/giacomo/ \\\n",
    "    --val-size 0.1 \\\n",
    "    --test-size 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing text\n",
    "\n",
    "The script above, `get_data.py`, also generates another field per each datapoint:\n",
    "- `normalized_text`: normalized text via custom NeMo's text normalizer for Italian language:\n",
    "    ```\n",
    "    nemo_text_processing.text_normalization.normalize.Normalizer(lang=\"it\", input_case=\"cased\", overwrite_cache=True, cache_dir=str(file_path / \"cache_dir\"))\n",
    "    ```\n",
    "    [github nemo IT](https://github.com/NVIDIA/NeMo-text-processing/tree/main/nemo_text_processing/text_normalization/it)\n",
    "    \n",
    "Here are some example records:\n",
    "```json\n",
    "\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(python my_phonemizer.py \\\n",
    "    --manifests /home/giacomo/il_fu_mattia_pascal/test_manifest.json /home/giacomo/il_fu_mattia_pascal/val_manifest.json /home/giacomo/il_fu_mattia_pascal/train_manifest.json \\\n",
    "    --language it \\\n",
    "    --preserve-punctuation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the phonemize method, refer to the docs [here](https://github.com/bootphon/phonemizer/blob/master/phonemizer/backend/base.py#L137).\n",
    "\n",
    " `my_phonemizer.py` generates `train_manifest_phonemes.json`, `test_manifest_phonemes.json` and `val_manifest_phonemes.json` respectively.\n",
    "\n",
    "We are effectively doubling the size of our dataset. Each original record maps on to two records, one with original `normalized_text` field value and `is_phoneme` set to 0 and another with phonemized text and `is_phoneme` flag set to 1.\n",
    "\n",
    "Example:\n",
    "```json\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"is_phoneme\": 0}\n",
    "\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 n\\u0254! ora! \\u2014 ribat\\u02d0e kwe\\u028e\\u026a, affer\\u027eandole \\u028an brat\\u0283\\u02d0o e at\\u02d0irandola a se.\", \"is_phoneme\": 1}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating supplementary data\n",
    "\n",
    "To accelerate and stabilize our training, we also need to extract pitch for every audio, estimate pitch statistics (mean and std) and pre-calculate alignment prior matrices for alignment framework. To do this, all we need to do is iterate over our data one time.\n",
    "\n",
    "In the below method the arguments are as follows:\n",
    "- `sup_data_path` — path to the folder which contains supplementary data. If the supplementary data or the folder does not already exists then it will be created.\n",
    "\n",
    "- `sup_data_types` — types of supplementary data to be provided to the model.\n",
    "\n",
    "- `text_tokenizer` — text tokenizer object that we already created.\n",
    "\n",
    "- `text_normalizer` — text normalizer object that we already created.\n",
    "\n",
    "- `text_normalizer_call_kwargs` — dictionary of arguments to be used in calling the text normalizer that we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python extract_sup_data.py \\\n",
    "        --config-path . \\\n",
    "        --config-name ds_for_fastpitch_align.yaml \\\n",
    "        ++dataloader_params.num_workers=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malavoglia:\n",
    "\n",
    "PITCH_MEAN=188.20228576660156, PITCH_STD=60.07517623901367\n",
    "\n",
    "PITCH_MIN=65.4063949584961, PITCH_MAX=2057.0478515625\n",
    "\n",
    "Il fu Mattia Pascal:\n",
    "\n",
    "PITCH_MEAN=159.78489685058594, PITCH_STD=31.194135665893555\n",
    "\n",
    "PITCH_MIN=65.4063949584961, PITCH_MAX=651.6829223632812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "# Text normalizer\n",
    "text_normalizer = Normalizer(\n",
    "    lang=\"it\", \n",
    "    input_case=\"cased\", \n",
    "    whitelist=\"/home/giacomo/NeMo-text-processing/nemo_text_processing/text_normalization/it/data/whitelist.tsv\"\n",
    ")\n",
    "\n",
    "text_normalizer_call_kwargs = {\n",
    "    \"punct_pre_process\": True,\n",
    "    \"punct_post_process\": True\n",
    "}\n",
    "\n",
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianPhonemesTokenizer\n",
    "# Text tokenizer\n",
    "text_tokenizer = ItalianPhonemesTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "normalizer = Normalizer(input_case='cased', lang='it')\n",
    "written = \"2 km/m dip. Fisica\"\n",
    "norm_it = normalizer.normalize(written, punct_post_process=True, verbose=True)\n",
    "print(norm_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianPhonemesTokenizer\n",
    "tokenizer = ItalianPhonemesTokenizer()\n",
    "text = \"E dunque? Ci sono poi tanti mezzi: di controllo!\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianCharsTokenizer\n",
    "tokenizer = ItalianCharsTokenizer()\n",
    "text = \"E dunque? Ci sono poi tanti mezzi: di controllo!\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calculate_supplementary_data(sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs):\n",
    "    # init train and val dataloaders\n",
    "    stages = [\"train\", \"val\"]\n",
    "    stage2dl = {}\n",
    "    for stage in stages:\n",
    "        ds = TTSDataset(\n",
    "            manifest_filepath=f\"/home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/malavoglia/{stage}_manifest_phonemes.json\",\n",
    "            sample_rate=16000,\n",
    "            sup_data_path=sup_data_path,\n",
    "            sup_data_types=sup_data_types,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            window=\"hann\",\n",
    "            n_mels=80,\n",
    "            lowfreq=0,\n",
    "            highfreq=8000,\n",
    "            text_tokenizer=text_tokenizer,\n",
    "            text_normalizer=text_normalizer,\n",
    "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
    "\n",
    "        ) \n",
    "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=1, collate_fn=ds._collate_fn, num_workers=1)\n",
    "\n",
    "    # iteration over dataloaders\n",
    "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
    "    for stage, dl in stage2dl.items():\n",
    "        pitch_list = []\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
    "            pitch = pitches.squeeze(0)\n",
    "            pitch_list.append(pitch[pitch != 0])\n",
    "\n",
    "        if stage == \"train\":\n",
    "            pitch_tensor = torch.cat(pitch_list)\n",
    "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
    "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
    "            \n",
    "    return pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
    "\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
    "    fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
    ")\n",
    "print(pitch_mean, pitch_std, pitch_min, pitch_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_max = 651.6829223632812\n",
    "pitch_min = 65.4063949584961\n",
    "pitch_mean = 159.78488159179688\n",
    "pitch_std = 31.194143295288086\n",
    "\n",
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this also via `extract_sup_data.py` script."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, the script results in something similar, where all default parameters are set in fastpitch_align.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-12 23:35:24 nemo_logging:349] /home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-12 23:35:25 exp_manager:384] Experiments will be logged at /home/giacomo/il_fu_mattia_pascal/checkpoint/FastPitch/2023-10-12_23-35-25\n",
      "[NeMo I 2023-10-12 23:35:25 exp_manager:823] TensorboardLogger has been set up\n",
      "Creating ClassifyFst grammars. This might take some time...\n",
      "[NeMo I 2023-10-12 23:35:42 dataset:228] Loading dataset from /home/giacomo/il_fu_mattia_pascal/train_manifest_phonemes.json.\n",
      "0it [00:00, ?it/s][NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e che– povero impiegato– aveva vissuto sempre lontano dalla famiglia, un po' qua, un po' là.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e che– povero impiegato– aveva vissuto sempre lontano dalla famiglia, un po' qua, un po' là.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e quella, sbattendo man mano più forte «ma sì! – ma certo! – ma come no? – ma sicuramente!»;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e quella, sbattendo man mano più forte «ma sì! – ma certo! – ma come no? – ma sicuramente!»;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e quella, sbattendo man mano più forte «ma sì! – ma certo! – ma come no? – ma sicuramente!»;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e condannavo– oh suprema irrisione! – a subir quello che non gli apparteneva falso compianto] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:42 tts_tokenizers:429] Text: [e condannavo– oh suprema irrisione! – a subir quello che non gli apparteneva falso compianto] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [da rompere la segreta armonia, che già– non so come– s'era tra noi stabilita.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [da rompere la segreta armonia, che già– non so come– s'era tra noi stabilita.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [del resto, la mamma– diceva– si sarebbe forse trovata male allo stesso modo in casa sua] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [del resto, la mamma– diceva– si sarebbe forse trovata male allo stesso modo in casa sua] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [noi, – non so se questo possa farle piacere– noi abbiamo sempre vissuto e sempre vivremo con l'universo;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [noi, – non so se questo possa farle piacere– noi abbiamo sempre vissuto e sempre vivremo con l'universo;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [che– vedendomi da un pezzo sul ponte– si fosse fermata a spiarmi.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [che– vedendomi da un pezzo sul ponte– si fosse fermata a spiarmi.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [che colpa ha lui, se io, – poi, – ingrato e sconoscente, andai a guastargli le uova nel paniere?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [che colpa ha lui, se io, – poi, – ingrato e sconoscente, andai a guastargli le uova nel paniere?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– poiché l'avevo allato– me l'ero preso;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– poiché l'avevo allato– me l'ero preso;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [i ricordo nen ben, perché mi' i l'hai nen conôssulo.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— aquì està! aquì está! — si mise a gridare subito pepita ridendo.] contains unknown char: [á]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– sotto– fors'anche meglio.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– sotto– fors'anche meglio.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ora– timido come un cane bastonato– andava appresso a quell'altro me che aveva aperte le finestre e si destava alla luce del giorno, accigliato, severo, impetuoso;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ora– timido come un cane bastonato– andava appresso a quell'altro me che aveva aperte le finestre e si destava alla luce del giorno, accigliato, severo, impetuoso;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– sarebbe prudente non precisare alcun luogo di nascita. come si fa?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [tuttavia– avendo notizia delle scene ch'erano avvenute e avvenivano in casa malagna] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma, via, credi davvero– soggiunsi, – che vorrò darti fastidio, se romilda non vuole?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma, via, credi davvero– soggiunsi, – che vorrò darti fastidio, se romilda non vuole?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [andate a innamorarvi del numero dodici! prima di tentare la sorte– benché senz'alcuna illusione– volli stare un pezzo a osservare] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [andate a innamorarvi del numero dodici! prima di tentare la sorte– benché senz'alcuna illusione– volli stare un pezzo a osservare] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e poi, se– vestito di questi stessi panni– quel tedescaccio in prima aveva potuto prendermi per un babbeo] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e poi, se– vestito di questi stessi panni– quel tedescaccio in prima aveva potuto prendermi per un babbeo] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [avevo anche quattrini, che– oltre al resto– forniscono pure certe idee, le quali senza di essi non si avrebbero.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [avevo anche quattrini, che– oltre al resto– forniscono pure certe idee, le quali senza di essi non si avrebbero.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [poi servirono per me; e furono– come dirò– la cagione della mia prima morte.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [poi servirono per me; e furono– come dirò– la cagione della mia prima morte.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [che pure– mi dissero– aveva due anni avanti mostrato una gran pena per il mio barbaro suicidio.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [che pure– mi dissero– aveva due anni avanti mostrato una gran pena per il mio barbaro suicidio.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– pensai che il sospetto di quella serva potesse in qualche modo esser fondato] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e anche– perché no?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– posatamente sforzandomi di vedere e di fissar bene tutto, nelle più minute particolarità.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma che! adriana– e ora lo intendo bene– non poteva assolutamente permettere che io tacessi e obbligassi anche lei al silenzio] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma che! adriana– e ora lo intendo bene– non poteva assolutamente permettere che io tacessi e obbligassi anche lei al silenzio] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ebbi un brivido nuovo, un tremor di tenerezza, ineffabile: – erano mie!] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [quello spagnoletto barbuto e atticciato voleva a ogni costo trattenermi– ecco: erano le undici e un quarto;] contains unknown char: [–]. Symbol will be skipped.\n",
      "2881it [00:00, 28802.27it/s][NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e ora, ecco– ah che rabbia! – li avrei liberati io...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e ora, ecco– ah che rabbia! – li avrei liberati io...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [quei libri recavano titoli di questo genere: la mort et l'au-delà– l'homme et ses corps] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— 'i veui nen côtradite: sarà prô paôlo.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— 'i veui nen côtradite: sarà prô paôlo.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— 'i veui nen côtradite: sarà prô paôlo.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— a m'smiava antôni, — disse stropicciandosi il mento ispido d'una barba di quattro giorni almeno, quasi tutta grigia.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– ma lèvati di qua: non ti conosco —» fremevo.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [questa volta, però, – debbo dirlo– la mia foga proveniva anche dal desiderio di sfondare la trista ragna ordita da quel laido vecchio] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [questa volta, però, – debbo dirlo– la mia foga proveniva anche dal desiderio di sfondare la trista ragna ordita da quel laido vecchio] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [a torino, – aggiunse piano), del quale volle anche mostrarmi il ritratto.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e, così armati, andammo cautamente a lui, gli accostammo la canna alle nari– e zifff! –.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e, così armati, andammo cautamente a lui, gli accostammo la canna alle nari– e zifff! –.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [era molto bravo poi, ingegnoso– forse un pochino bisbetico e volubile– ma con vedute sue, originali;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [era molto bravo poi, ingegnoso– forse un pochino bisbetico e volubile– ma con vedute sue, originali;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e agradecio dio, ántes, che me la son levada de sobre!] contains unknown char: [á]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– les sept principes de l'homme– karma– la clef de la théosophie– a b c de la théosophie– la doctrine secrète– le plan astral– eccetera, eccetera.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [egli– era già inteso– non sapeva e non doveva saper nulla di quel furto, e io, con quella mia affermazione, non salvavo che suo fratello] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [egli– era già inteso– non sapeva e non doveva saper nulla di quel furto, e io, con quella mia affermazione, non salvavo che suo fratello] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [lo sbaglio di sua moglie, sbaglio che poté anche– non nego– essere in mala fede.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [lo sbaglio di sua moglie, sbaglio che poté anche– non nego– essere in mala fede.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [nel mio povero e timidissimo francese, volli fargli notare che aveva sbagliato– oh, certo involontariamente! era un tedesco] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e mi tenne la mano fin sulla soglia dell'uscio, come se temesse ancora, che– lasciandomi per un momento– io potessi sparir di nuovo.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e mi tenne la mano fin sulla soglia dell'uscio, come se temesse ancora, che– lasciandomi per un momento– io potessi sparir di nuovo.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ogni minimo che– sospeso come già da un pezzo mi sentivo in un vuoto strano– mi faceva ora cadere in lunghe riflessioni.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ogni minimo che– sospeso come già da un pezzo mi sentivo in un vuoto strano– mi faceva ora cadere in lunghe riflessioni.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [feci per gittarlo al fiume, ma– sul punto– un'idea mi balenò;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [feci per gittarlo al fiume, ma– sul punto– un'idea mi balenò;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– dalla speranza di fare un bene a quella ragazza che veramente mi aveva fatto una grande impressione.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [va' piuttosto a leggere anche tu che birnbaum giovanni abramo fece stampare a lipsia nel mille settecento trentotto un opuscolo in–ottavo:] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e se no... avevo sentito dire che non difettavano alberi– solidi– nel giardino attorno alla bisca.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e se no... avevo sentito dire che non difettavano alberi– solidi– nel giardino attorno alla bisca.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e nui sôma cusin.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [io mi ero tolto– bene o male– il pensiero più fastidioso e più affliggente che si possa avere, vivendo: quello della morte.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [io mi ero tolto– bene o male– il pensiero più fastidioso e più affliggente che si possa avere, vivendo: quello della morte.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e per farci morire– spesso con la coscienza d'aver commesso una sequela di piccole sciocchezze– dopo cinquanta o sessanta giri?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e per farci morire– spesso con la coscienza d'aver commesso una sequela di piccole sciocchezze– dopo cinquanta o sessanta giri?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [malagna s'intenerì– ma fino a un certo segno.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [poi– rubava...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [cresceva, ribolliva, scoppiava: – e io, ancora lì, zitto! a un certo punto] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– b) nato in america nell'argentina, senz'altra designazione;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [sì: ma– a dir vero– non ce ne fu molta di più per noi nelle sere successive, rispetto allo spiritismo, s'intende.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [sì: ma– a dir vero– non ce ne fu molta di più per noi nelle sere successive, rispetto allo spiritismo, s'intende.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [il guajo fu, quando– dopo essermi liberato di tutti quei capellacci– mi rimisi in capo il cappello comperato poc'anzi: mi sprofondò fin su la nuca! dovetti rimediare] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [il guajo fu, quando– dopo essermi liberato di tutti quei capellacci– mi rimisi in capo il cappello comperato poc'anzi: mi sprofondò fin su la nuca! dovetti rimediare] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e no! non andava bene neanche così: né per me, né per la signorina caporale, né per adriana e né– come si vide poco dopo– per la pepita] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e no! non andava bene neanche così: né per me, né per la signorina caporale, né per adriana e né– come si vide poco dopo– per la pepita] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [un'eternità mi pareva, e che– come erano accaduti a me casi straordinarii– dovessero parimenti esserne accaduti a miragno.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [un'eternità mi pareva, e che– come erano accaduti a me casi straordinarii– dovessero parimenti esserne accaduti a miragno.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [mi aveva dato il suo biglietto da visita: – cavalier tito lenzi.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non parlava, forse perché non stimava dover suo parlare, o perché– com'io ritengo più probabile– ne godeva in segreto, velenosamente.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non parlava, forse perché non stimava dover suo parlare, o perché– com'io ritengo più probabile– ne godeva in segreto, velenosamente.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– appena seduto– ponendosi il bastone tra le gambe, trasse un profondo respiro e sorrise alla sua stanchezza mortale.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– appena seduto– ponendosi il bastone tra le gambe, trasse un profondo respiro e sorrise alla sua stanchezza mortale.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [a vɛʊl dɪ ke tɪ tɪ ɛsseeːɛsse fjɛʊl d̪iː barba antonɪ ka le andaɪt ntla amɛrika.] contains unknown char: [̪]. Symbol will be skipped.\n",
      "5762it [00:00, 27881.38it/s][NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma se mi provano, perdiana, che– dopo averla sopportata per altri cinque o sei o dieci anni– io non avrò pagato lo scotto in qualche modo] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma se mi provano, perdiana, che– dopo averla sopportata per altri cinque o sei o dieci anni– io non avrò pagato lo scotto in qualche modo] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [la mia fortuna– dovevo convincermene– la mia fortuna consisteva appunto in questo: nell'essermi liberato della moglie] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [la mia fortuna– dovevo convincermene– la mia fortuna consisteva appunto in questo: nell'essermi liberato della moglie] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— l'è propi për lon che mi't son vnù a trôvè.] contains unknown char: [ë]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— l'è propi për lon che mi't son vnù a trôvè.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non diceva nulla della dote: – vistosissima! – tutta la sostanza del marchese d'auletta, nientemeno.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non diceva nulla della dote: – vistosissima! – tutta la sostanza del marchese d'auletta, nientemeno.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [tesi l'orecchio, con l'idea di fuggire non appena quei due– papiano e lo spagnuolo (era lui] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– comprendendo ch'ella era venuta con la scusa di quella nota per aver da me una parola che la raffermasse nelle sue speranze;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [innanzi a gli altri, un tratto confidenziale– mi parlò piano, affrettatamente.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— dôva ca l'è stô me car parent?] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— dôva ca l'è stô me car parent?] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [» il paleari intanto, che– solo– non aveva provato né meraviglia né sgomento] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [» il paleari intanto, che– solo– non aveva provato né meraviglia né sgomento] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [figuriamoci! – è lui, è lui! mio genero! ah, povero mattia! ah, povero figliuolo mio! – e si sarà messa a piangere fors'anche;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [figuriamoci! – è lui, è lui! mio genero! ah, povero mattia! ah, povero figliuolo mio! – e si sarà messa a piangere fors'anche;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [mentre, per esempio, stava a parlar con me, s'accorgeva– non so come– che adriana, dietro a lui] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [mentre, per esempio, stava a parlar con me, s'accorgeva– non so come– che adriana, dietro a lui] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– sarò tale al giuoco;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [a oliva era nato fin dal primo anno il sospetto che, via, tra lui e lei– come dire?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [già– ripeto– son come fuori della vita, e non m'importa più di nulla.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [già– ripeto– son come fuori della vita, e non m'importa più di nulla.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [al bujo– era colpa mia?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– molti forse avrebbero ritegno a confessarlo;] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non tanto– disse– per rifarmi le scuse del modo poco decente in cui mi era apparso la prima volta] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non tanto– disse– per rifarmi le scuse del modo poco decente in cui mi era apparso la prima volta] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [tanto che io– per riparare in certo qual modo alla nera ingratitudine de' miei concittadini– potei tracciarvi a grosse lettere questa iscrizione:] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [tanto che io– per riparare in certo qual modo alla nera ingratitudine de' miei concittadini– potei tracciarvi a grosse lettere questa iscrizione:] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— tut i meis i sôma parent.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e allora, lei, romilda, piangendo– dice– a calde lagrime, si gittò ai piedi di lui] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e allora, lei, romilda, piangendo– dice– a calde lagrime, si gittò ai piedi di lui] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– oh! poniamo ora che veramente ella sia esposta] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [si può essere– domando io– più onesti di così?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [si può essere– domando io– più onesti di così?] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [come se– alle grida– si fosse levata di letto in fretta e in furia, si fece innanzi] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [come se– alle grida– si fosse levata di letto in fretta e in furia, si fece innanzi] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [ma è pure umano, umano, umano– io non sentii pena, no] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [la avrei conosciuta– diceva– fra qualche sera, perché egli la avrebbe indotta a intervenire alle prossime sedute spiritiche.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [la avrei conosciuta– diceva– fra qualche sera, perché egli la avrebbe indotta a intervenire alle prossime sedute spiritiche.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [a cui– ecco– sotto, su i quadrati gialli del tavoliere, tante mani avevano recato, come in offerta votiva, oro, oro e oro] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [a cui– ecco– sotto, su i quadrati gialli del tavoliere, tante mani avevano recato, come in offerta votiva, oro, oro e oro] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [parola volgare, che– da subalterno– non stimai conveniente sottoporre agli occhi d'un assessore comunale per la pubblica istruzione.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [parola volgare, che– da subalterno– non stimai conveniente sottoporre agli occhi d'un assessore comunale per la pubblica istruzione.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [due giorni dopo, mandata– suppongo– da margherita, venne in gran furia, al solito, zia scolastica, per portarsi via con sé la mamma.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [due giorni dopo, mandata– suppongo– da margherita, venne in gran furia, al solito, zia scolastica, per portarsi via con sé la mamma.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– su la parrucca– un ampio fazzoletto di seta cilestrina, anzi uno scialle, annodato artisticamente sotto il mento.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e– su la parrucca– un ampio fazzoletto di seta cilestrina, anzi uno scialle, annodato artisticamente sotto il mento.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [in tutto il resto, no di certo– facciamo così: senza patti fra noi e senza alcuna responsabilità da parte mia,] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [preparando, non smise– s'intende! – un solo istante di parlare.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [preparando, non smise– s'intende! – un solo istante di parlare.] contains unknown char: [–]. Symbol will be skipped.\n",
      "8152it [00:00, 27211.18it/s]\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:266] Loaded dataset with 8152 files.\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:268] Dataset contains 12.99 hours.\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:376] Pruned 0 files. Final dataset contains 8152 files\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:378] Pruned 0.00 hours. Final dataset contains 12.99 hours.\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:228] Loading dataset from /home/giacomo/il_fu_mattia_pascal/val_manifest_phonemes.json.\n",
      "0it [00:00, ?it/s][NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– mio figlio! ella mi ha guardato con occhi affettuosi e ridenti, che m'han detto in un baleno tante cose...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [lei sola: – non certo quelli che la guardavano, sospesi nel supplizio che cagionava loro il capriccio di essa] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [chi? il tavolino! quattro colpi: – bujo! giuro di non averli sentiti.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [sì: ma io– era chiaro– io non avevo voluto: avevo preferito di sacrificar così dodici mila lire...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [sì: ma io– era chiaro– io non avevo voluto: avevo preferito di sacrificar così dodici mila lire...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [sarebbe un peccato, via! perché– spiacevole quanto si voglia quest'incidente– i fenomeni accennavano questa sera a manifestarsi con insolita energia.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [sarebbe un peccato, via! perché– spiacevole quanto si voglia quest'incidente– i fenomeni accennavano questa sera a manifestarsi con insolita energia.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e io me n'ero approfittato! più d'una volta, al bujo– lo confesso– gelai di paura.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [e io me n'ero approfittato! più d'una volta, al bujo– lo confesso– gelai di paura.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [si trovavano nella mia identica condizione, nei «gusci» del kâmaloka, specialmente i suicidi, che il signor leadbeater] contains unknown char: [â]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [tanto che, circa tre mesi addietro, già una prima volta, di notte tempo, egli aveva tentato di pôr fine a' suoi miseri giorni] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [io ora sono vivo– vedi? – e voglio stare allegro...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [io ora sono vivo– vedi? – e voglio stare allegro...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— agradecio dio, ántes che me la son levada de sobre!] contains unknown char: [á]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [lui– eh già! – la aveva amata prima.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [lui– eh già! – la aveva amata prima.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [— oh ma côsta ca l'è bela! — esclamò colui.] contains unknown char: [ô]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– l'ho qua– una lettera per il direttore di una casa di salute a napoli, dove devo recarmi anche per altri documenti che gli bisognano...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– l'ho qua– una lettera per il direttore di una casa di salute a napoli, dove devo recarmi anche per altri documenti che gli bisognano...] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [– la mancanza potesse più esser di lui che sua, non ostante che egli si ostinasse a dir di no.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non così zia scolastica, la quale– non riuscendo ad appioppare a mia madre il suo prediletto pomino– s'era messa a perseguitar berto e me.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [non così zia scolastica, la quale– non riuscendo ad appioppare a mia madre il suo prediletto pomino– s'era messa a perseguitar berto e me.] contains unknown char: [–]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-12 23:35:43 tts_tokenizers:429] Text: [mio padre... – a proposito, come si chiamava?] contains unknown char: [–]. Symbol will be skipped.\n",
      "1166it [00:00, 26745.56it/s]\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:266] Loaded dataset with 1166 files.\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:268] Dataset contains 1.86 hours.\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:376] Pruned 0 files. Final dataset contains 1166 files\n",
      "[NeMo I 2023-10-12 23:35:43 dataset:378] Pruned 0.00 hours. Final dataset contains 1.86 hours.\n",
      "[NeMo I 2023-10-12 23:35:43 features:289] PADDING: 1\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-10-12 23:35:45 modelPT:728] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        capturable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.001\n",
      "        maximize: False\n",
      "        weight_decay: 1e-06\n",
      "    )\n",
      "[NeMo I 2023-10-12 23:35:45 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.NoamAnnealing object at 0x7f3db80abca0>\" \n",
      "    will be used during training (effective maximum steps = -255) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 1000\n",
      "    last_epoch: -1\n",
      "    d_model: 1\n",
      "    max_steps: -255\n",
      "    )\n",
      "\n",
      "  | Name                | Type                              | Params\n",
      "--------------------------------------------------------------------------\n",
      "0 | mel_loss_fn         | MelLoss                           | 0     \n",
      "1 | pitch_loss_fn       | PitchLoss                         | 0     \n",
      "2 | duration_loss_fn    | DurationLoss                      | 0     \n",
      "3 | energy_loss_fn      | EnergyLoss                        | 0     \n",
      "4 | aligner             | AlignmentEncoder                  | 1.0 M \n",
      "5 | forward_sum_loss_fn | ForwardSumLoss                    | 0     \n",
      "6 | bin_loss_fn         | BinLoss                           | 0     \n",
      "7 | preprocessor        | AudioToMelSpectrogramPreprocessor | 0     \n",
      "8 | fastpitch           | FastPitchModule                   | 45.8 M\n",
      "--------------------------------------------------------------------------\n",
      "45.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.8 M    Total params\n",
      "183.103   Total estimated model params size (MB)\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2023-10-12 23:35:45 nemo_logging:349] /home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-12 23:36:18 nemo_logging:349] /home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0:   0%|                                          | 0/255 [00:00<?, ?it/s][NeMo W 2023-10-12 23:36:45 nemo_logging:349] /home/giacomo/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "Epoch 0:   0%| | 1/255 [00:27<1:55:23, 27.26s/it, v_num=5-25, train_step_timing Reducer buckets have been rebuilt in this iteration.\n",
      "Epoch 0:  11%| | 28/255 [06:00<48:39, 12.86s/it, v_num=5-25, train_step_timing iError executing job with overrides: ['sample_rate=16000', 'train_dataset=/home/giacomo/il_fu_mattia_pascal/train_manifest_phonemes.json', 'validation_datasets=/home/giacomo/il_fu_mattia_pascal/val_manifest_phonemes.json', 'sup_data_path=sup_data', 'exp_manager.exp_dir=/home/giacomo/il_fu_mattia_pascal/checkpoint', 'trainer.check_val_every_n_epoch=1']\n",
      "Traceback (most recent call last):\n",
      "  File \"fastpitch.py\", line 31, in main\n",
      "    trainer.fit(model)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 532, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 93, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 571, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 980, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 355, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 133, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 219, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 188, in run\n",
      "    self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 266, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 146, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/core/module.py\", line 1276, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 161, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py\", line 257, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 231, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/amp.py\", line 76, in optimizer_step\n",
      "    closure_result = closure()\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 142, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/giacomo/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 137, in closure\n",
      "    self._backward_fn(step_output.closure_loss)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 237, in backward_fn\n",
      "    call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 294, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 205, in backward\n",
      "    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 69, in backward\n",
      "    model.backward(tensor, *args, **kwargs)\n",
      "  File \"/home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/pytorch_lightning/core/module.py\", line 1064, in backward\n",
      "    loss.backward(*args, **kwargs)\n",
      "  File \"/home/giacomo/.local/lib/python3.8/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/giacomo/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.51 GiB (GPU 0; 4.00 GiB total capacity; 5.60 GiB already allocated; 0 bytes free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "Epoch 0:  11%| | 28/255 [06:28<52:28, 13.87s/it, v_num=5-25, train_step_timing i\n"
     ]
    }
   ],
   "source": [
    "!(CUDA_VISIBLE_DEVICES=0 python fastpitch.py --config-path . --config-name=fastpitch_align_ITA.yaml \\\n",
    "  sample_rate=16000 \\\n",
    "  train_dataset=/home/giacomo/il_fu_mattia_pascal/train_manifest_phonemes.json \\\n",
    "  validation_datasets=/home/giacomo/il_fu_mattia_pascal/val_manifest_phonemes.json \\\n",
    "  sup_data_path=sup_data \\\n",
    "  exp_manager.exp_dir=/home/giacomo/il_fu_mattia_pascal/checkpoint \\\n",
    "  trainer.check_val_every_n_epoch=1 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. We use `CUDA_VISIBLE_DEVICES=0` to limit training to single GPU.\n",
    "2. For debugging you may also add the following flags: `HYDRA_FULL_ERROR=1`, `CUDA_LAUNCH_BLOCKING=1`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating FastPitch + pretrained HiFi-GAN\n",
    "\n",
    "Let's evaluate the quality of the FastPitch model generated so far using a HiFi-GAN model pre-trained on English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from nemo.collections.tts.models import HifiGanModel, FastPitchModel\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"E non le pare che fosse rosso, ad esempio, il lanternone della Virt\\u00f9 pagana?\" # text input to the model\n",
    "test_id = \"mattiapascal_12_pirandello3_f000058\" # identifier for the audio corresponding to the test text\n",
    "data_path = \"/home/giacomo/il_fu_mattia_pascal/wavs/\" # path to dataset folder with wav files from original dataset\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spec_fastpitch_ckpt(spec_gen_model, v_model, test):\n",
    "    with torch.no_grad():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        parsed = spec_gen_model.parse(str_input=test, normalize=True)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed)\n",
    "        print(spectrogram.size())\n",
    "        audio = v_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "    spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "    audio = audio.to('cpu').numpy()[0]\n",
    "    audio = audio / np.abs(audio).max()\n",
    "    return audio, spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hifigan models\n",
    "hfg_ngc = \"tts_en_lj_hifigan_ft_mixerttsx\" # NGC pretrained model name: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_lj_hifigan \n",
    "vocoder_model = HifiGanModel.from_pretrained(hfg_ngc, strict=False).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fastpitch\n",
    "import glob, os\n",
    "fastpitch_model_path = sorted(\n",
    "    glob.glob(\"FastPitch.ckpt\"), \n",
    "    key=os.path.getmtime)[-1] # path_to_fastpitch_nemo_or_ckpt\n",
    "\n",
    "if \".nemo\" in fastpitch_model_path:\n",
    "    spec_gen_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()\n",
    "else:\n",
    "    spec_gen_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model, test)\n",
    "\n",
    "# visualize the spectrogram\n",
    "if spectrogram is not None:\n",
    "    imshow(spectrogram, origin=\"lower\")\n",
    "    plt.show()\n",
    "\n",
    "# audio\n",
    "print(\"original audio\")\n",
    "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=16000))\n",
    "print(\"predicted audio\")\n",
    "ipd.display(ipd.Audio(audio, rate=16000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning HiFi-GAN\n",
    "\n",
    "Improving speech quality by Finetuning HiFi-GAN on synthesized mel-spectrograms from FastPitch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_text = \"E non le pare che fosse rosso, ad esempio, il lanternone della Virt\\u00f9 pagana?\"\n",
    "test_audio_filepath = \"/home/giacomo/il_fu_mattia_pascal/wavs/mattiapascal_12_pirandello3_f000058.wav\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "\n",
    "def load_wav(audio_file):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "    return samples.transpose()\n",
    "\n",
    "def plot_logspec(spec, axis=None):    \n",
    "    librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(spec, ref=np.max),\n",
    "        y_axis='linear', \n",
    "        x_axis=\"time\",\n",
    "        fmin=0, \n",
    "        fmax=8000,\n",
    "        ax=axis\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original mel spectrogram generated from original audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading original melspec\")\n",
    "y, sr = librosa.load(test_audio_filepath)\n",
    "# change n_fft, win_length, hop_length parameters below based on your specific config file\n",
    "spectrogram2 = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, win_length=1024, hop_length=256)\n",
    "spectrogram = spectrogram2[ :80, :]\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plot_logspec(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel spectrogram predicted from FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading fastpitch melspec via generate_spectrogram\")\n",
    "with torch.no_grad():\n",
    "    text = spec_model.parse(test_audio_text, normalize=False)\n",
    "    spectrogram = spec_model.generate_spectrogram(\n",
    "      tokens=text, \n",
    "      speaker=None,\n",
    "    )\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "plot_logspec(spectrogram)\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above predicted spectrogram has the duration lower in frames which is not equal to the ground truth 498 frames. In order to finetune HiFi-GAN we need mel spectrogram predicted from FastPitch with ground truth alignment and duration.\n",
    "\n",
    "### Mel spectrogram predicted from FastPitch with groundtruth alignment and duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading fastpitch melspec via forward method with groundtruth alignment and duration\")\n",
    "with torch.no_grad():\n",
    "    device = spec_model.device\n",
    "    beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "    text = spec_model.parse(test_audio_text, normalize=False)\n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    audio = load_wav(test_audio_filepath)\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "    attn_prior = torch.from_numpy(\n",
    "      beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "    ).unsqueeze(0).to(text.device)\n",
    "    spectrogram = spec_model.forward(\n",
    "      text=text, \n",
    "      input_lens=text_len, \n",
    "      spec=spect, \n",
    "      mel_lens=spect_len, \n",
    "      attn_prior=attn_prior,\n",
    "      speaker=None,\n",
    "    )[0]\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plot_logspec(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finetuning without groundtruth alignment and duration has artifacts from the original audio (noise) that get passed on as input to the vocoder resulting in artifacts in vocoder output in the form of noise.\n",
    "- <b> On the other hand, `Mel spectrogram predicted from FastPitch with groundtruth alignment and duration` gives the best results because it enables HiFi-GAN to learn mel spectrograms generated by FastPitch as well as duration distributions closer to the real world (i.e. ground truth) durations. </b>\n",
    "\n",
    "From implementation perspective - we follow the same process described in [Finetuning FastPitch for a new speaker](FastPitch_Finetuning.ipynb) - i.e. take the latest checkpoint from FastPitch training and predict spectrograms for each of the input records in `train_manifest_text_normed.json`, `test_manifest_text_normed.json` and `val_manifest_text_normed.json`. NeMo provides an efficient script, [scripts/dataset_processing/tts/generate_mels.py](https://raw.githubusercontent.com/nvidia/NeMo/main/scripts/dataset_processing/tts/generate_mels.py), to generate Mel-spectrograms in the directory `NeMoGermanTTS/mels` and also create new JSON manifests with a suffix `_mel` by adding a new key `\"mel_filepath\"`. For example, `train_manifest_text_normed.json` corresponds to `train_manifest_text_normed_mel.json` saved in the same directory. You can run the following CLI to obtain the new JSON manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_mels.py \\\n",
    "    --cpu \\\n",
    "    --input-json-manifests /home/giacomo/il_fu_mattia_pascal/train_manifest.json /home/giacomo/il_fu_mattia_pascal/test_manifest.json /home/giacomo/il_fu_mattia_pascal/val_manifest.json \\\n",
    "    --fastpitch-model-ckpt {fastpitch_model_path} \\\n",
    "    --output-json-manifest-root ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
