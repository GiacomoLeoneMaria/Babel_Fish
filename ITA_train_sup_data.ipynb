{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.data.dataset import TTSDataset\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nemo.collections.tts.models.base import SpectrogramGenerator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./reinstall.sh dev\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget text-unidecode scipy==1.7.3\n",
    "# !pip install phonemizer && apt-get update\n",
    "# apt-get install espeak-ng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tts/configs.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastPitch\n",
    "\n",
    "FastPitch is non-autoregressive model for mel-spectrogram generation based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference [paper](https://ieeexplore.ieee.org/abstract/document/9413889). \n",
    "\n",
    "### HiFiGAN\n",
    "\n",
    "HiFiGAN is a generative adversarial network (GAN) model that generates audio from mel spectrograms. The generator uses transposed convolutions to upsample mel spectrograms to audio [paper](https://arxiv.org/abs/2010.05646). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "* Creating manifests\n",
    "* Normalizing text\n",
    "* Phonemization\n",
    "* Creating supplementary data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating manifests \n",
    "\n",
    "I created the script `my_get_data.py` which reads the file `the_fu_mattia_pascal/metadata.csv` provided with the dataset and generates the following fields for each datapoint:\n",
    "1. `audio_filepath`: location of the wav file\n",
    "2. `duration`: duration of the wav file\n",
    "3. `text`: original text\n",
    "    \n",
    "After that, the script randomly splits the data into 3 buckets, `train_manifest.json`, `val_manifest.json` and `test_manifest.json`.\n",
    "\n",
    "Also `my_get_data_multi_speaker.py` works the same way, but for multiple datasets (generates multi speaker)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% datapoints go to validation set, 20% go to test set and the remaining 70% go to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python my_get_data.py \\\n",
    "    --data-root /home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/ \\\n",
    "    --val-size 0.1 \\\n",
    "    --test-size 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing text\n",
    "\n",
    "The script above, `get_data.py`, also generates another field per each datapoint:\n",
    "- `normalized_text`: normalized text via custom NeMo's text normalizer for Italian language:\n",
    "    ```\n",
    "    nemo_text_processing.text_normalization.normalize.Normalizer(lang=\"it\", input_case=\"cased\", overwrite_cache=True, cache_dir=str(file_path / \"cache_dir\"))\n",
    "    ```\n",
    "    [github nemo IT](https://github.com/NVIDIA/NeMo-text-processing/tree/main/nemo_text_processing/text_normalization/it)\n",
    "    \n",
    "Here are some example records:\n",
    "```json\n",
    "\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python my_phonemizer.py \\\n",
    "    --manifests /home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/malavoglia/test_manifest.json /home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/malavoglia/val_manifest.json /home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/malavoglia/train_manifest.json \\\n",
    "    --language it \\\n",
    "    --preserve-punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the phonemize method, refer to the docs [here](https://github.com/bootphon/phonemizer/blob/master/phonemizer/backend/base.py#L137).\n",
    "\n",
    " `my_phonemizer.py` generates `train_manifest_phonemes.json`, `test_manifest_phonemes.json` and `val_manifest_phonemes.json` respectively.\n",
    "\n",
    "We are effectively doubling the size of our dataset. Each original record maps on to two records, one with original `normalized_text` field value and `is_phoneme` set to 0 and another with phonemized text and `is_phoneme` flag set to 1.\n",
    "\n",
    "Example:\n",
    "```json\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"is_phoneme\": 0}\n",
    "\n",
    "{\"audio_filepath\": \"/home/giacomoleonemaria/NeMo/il_fu_mattia_pascal/wavs/mattiapascal_10_pirandello_f000400.wav\", \"duration\": 4.989813, \"text\": \"\\u2014 No! ora! \\u2014 ribatt\\u00e9 quegli, afferrandole un braccio e attirandola a s\\u00e9.\", \"normalized_text\": \"\\u2014 n\\u0254! ora! \\u2014 ribat\\u02d0e kwe\\u028e\\u026a, affer\\u027eandole \\u028an brat\\u0283\\u02d0o e at\\u02d0irandola a se.\", \"is_phoneme\": 1}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating supplementary data\n",
    "\n",
    "To accelerate and stabilize our training, we also need to extract pitch for every audio, estimate pitch statistics (mean and std) and pre-calculate alignment prior matrices for alignment framework. To do this, all we need to do is iterate over our data one time.\n",
    "\n",
    "In the below method the arguments are as follows:\n",
    "- `sup_data_path` — path to the folder which contains supplementary data. If the supplementary data or the folder does not already exists then it will be created.\n",
    "\n",
    "- `sup_data_types` — types of supplementary data to be provided to the model.\n",
    "\n",
    "- `text_tokenizer` — text tokenizer object that we already created.\n",
    "\n",
    "- `text_normalizer` — text normalizer object that we already created.\n",
    "\n",
    "- `text_normalizer_call_kwargs` — dictionary of arguments to be used in calling the text normalizer that we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python extract_sup_data.py \\\n",
    "        --config-path . \\\n",
    "        --config-name ds_for_fastpitch_align.yaml \\\n",
    "        ++dataloader_params.num_workers=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malavoglia:\n",
    "\n",
    "PITCH_MEAN=188.20228576660156, PITCH_STD=60.07517623901367\n",
    "\n",
    "PITCH_MIN=65.4063949584961, PITCH_MAX=2057.0478515625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "# Text normalizer\n",
    "text_normalizer = Normalizer(\n",
    "    lang=\"it\", \n",
    "    input_case=\"cased\", \n",
    "    whitelist=\"/home/giacomo/NeMo-text-processing/nemo_text_processing/text_normalization/it/data/whitelist.tsv\"\n",
    ")\n",
    "\n",
    "text_normalizer_call_kwargs = {\n",
    "    \"punct_pre_process\": True,\n",
    "    \"punct_post_process\": True\n",
    "}\n",
    "\n",
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianPhonemesTokenizer\n",
    "# Text tokenizer\n",
    "text_tokenizer = ItalianPhonemesTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "normalizer = Normalizer(input_case='cased', lang='it')\n",
    "written = \"2 km/m dip. Fisica\"\n",
    "norm_it = normalizer.normalize(written, punct_post_process=True, verbose=True)\n",
    "print(norm_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianPhonemesTokenizer\n",
    "tokenizer = ItalianPhonemesTokenizer()\n",
    "text = \"E dunque? Ci sono poi tanti mezzi: di controllo!\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import ItalianCharsTokenizer\n",
    "tokenizer = ItalianCharsTokenizer()\n",
    "text = \"E dunque? Ci sono poi tanti mezzi: di controllo!\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calculate_supplementary_data(sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs):\n",
    "    # init train and val dataloaders\n",
    "    stages = [\"train\", \"val\"]\n",
    "    stage2dl = {}\n",
    "    for stage in stages:\n",
    "        ds = TTSDataset(\n",
    "            manifest_filepath=f\"/home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/malavoglia/{stage}_manifest_phonemes.json\",\n",
    "            sample_rate=16000,\n",
    "            sup_data_path=sup_data_path,\n",
    "            sup_data_types=sup_data_types,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            window=\"hann\",\n",
    "            n_mels=80,\n",
    "            lowfreq=0,\n",
    "            highfreq=8000,\n",
    "            text_tokenizer=text_tokenizer,\n",
    "            text_normalizer=text_normalizer,\n",
    "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
    "\n",
    "        ) \n",
    "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=1, collate_fn=ds._collate_fn, num_workers=1)\n",
    "\n",
    "    # iteration over dataloaders\n",
    "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
    "    for stage, dl in stage2dl.items():\n",
    "        pitch_list = []\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
    "            pitch = pitches.squeeze(0)\n",
    "            pitch_list.append(pitch[pitch != 0])\n",
    "\n",
    "        if stage == \"train\":\n",
    "            pitch_tensor = torch.cat(pitch_list)\n",
    "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
    "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
    "            \n",
    "    return pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script should gives the following result:\n",
    "1. Creates two folders under `fastpitch_sup_data_folder` - `pitch` and `align_prior_matrix`\n",
    "2. Prints out the values for pitch_mean, pitch_std, pitch_min, pitch_max. Use these values while training FastPitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
    "\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
    "    fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pitch_mean, pitch_std, pitch_min, pitch_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pitch_max: 651.6829223632812\n",
    "* pitch_min: 65.4063949584961\n",
    "* pitch_mean: 159.78488159179688\n",
    "* pitch_std: 31.194143295288086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_max = 651.6829223632812\n",
    "pitch_min = 65.4063949584961\n",
    "pitch_mean = 159.78488159179688\n",
    "pitch_std = 31.194143295288086\n",
    "\n",
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this also via `extract_sup_data.py` script."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, the script results in something similar, where all default parameters are set in fastpitch_align.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-05 17:46:56 nemo_logging:349] /home/giacomo/anaconda3/envs/nemo/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-05 17:46:56 exp_manager:384] Experiments will be logged at resultITA_TTS/FastPitch/2023-10-05_17-46-56\n",
      "[NeMo I 2023-10-05 17:46:56 exp_manager:823] TensorboardLogger has been set up\n",
      "Creating ClassifyFst grammars. This might take some time...\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:228] Loading dataset from /home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/train_manifest_phonemes.json.\n",
      "0it [00:00, ?it/s][NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [sulla santa parola d’onore! pensava a tutt’altro lui, se non lo sapeva comare lia!….] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [sʊlla santa paɾɔla donore! pensava a tʊtːaltro lui, se non lo sapɛva komare lia!….] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [voi siete come le… e disse come.] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [voi sjete kome le… e disse kome.] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [… e noi, se arriviamo a ricomprare la casa del nespolo, quando ci avremo il grano nel graticcio] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [… e noi, se arɾivjamo a rikomprare la kaza del nɛspolo, kwando tʃɪ avrɛmo il ɡrano nel ɡratitʃːo] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [— i lupini? … non ce li abbiamo mangiati, i suoi lupini; non li abbiamo in tasca;] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [— i lʊpinɪ? … non tʃe lɪ abːjamo mandʒatɪ, i sʊɔi lʊpinɪ; non lɪ abːjamo in taska;] contains unknown char: […]. Symbol will be skipped.\n",
      "5410it [00:00, 26526.80it/s][NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [lì vicino, tanto che la zuppidda interruppe i vituperî che stava dicendo di lui per salutarlo.] contains unknown char: [î]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [sai il detto dell’antico che gli disse gesù cristo a san giovanni: «degli uomini segnati guárdatene!».] contains unknown char: [á]. Symbol will be skipped.\n",
      "8196it [00:00, 26867.59it/s]\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:266] Loaded dataset with 8196 files.\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:268] Dataset contains 11.75 hours.\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:376] Pruned 0 files. Final dataset contains 8196 files\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:378] Pruned 0.00 hours. Final dataset contains 11.75 hours.\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:228] Loading dataset from /home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/val_manifest_phonemes.json.\n",
      "0it [00:00, ?it/s][NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [tanto, oggi o domani!… e non si muoveva.] contains unknown char: […]. Symbol will be skipped.\n",
      "[NeMo W 2023-10-05 17:47:14 tts_tokenizers:429] Text: [tanto, ɔdʒːɪ o domanɪ!… e non sɪ mʊovɛva.] contains unknown char: […]. Symbol will be skipped.\n",
      "1172it [00:00, 16005.30it/s]\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:266] Loaded dataset with 1172 files.\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:268] Dataset contains 1.65 hours.\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:376] Pruned 0 files. Final dataset contains 1172 files\n",
      "[NeMo I 2023-10-05 17:47:14 dataset:378] Pruned 0.00 hours. Final dataset contains 1.65 hours.\n",
      "[NeMo I 2023-10-05 17:47:14 features:289] PADDING: 1\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-10-05 17:47:18 modelPT:728] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        capturable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.001\n",
      "        maximize: False\n",
      "        weight_decay: 1e-06\n",
      "    )\n",
      "[NeMo I 2023-10-05 17:47:18 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.NoamAnnealing object at 0x7feeb23bf370>\" \n",
      "    will be used during training (effective maximum steps = 2570000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 10\n",
      "    last_epoch: -1\n",
      "    d_model: 1\n",
      "    max_steps: 2570000\n",
      "    )\n",
      "\n",
      "  | Name                | Type                              | Params\n",
      "--------------------------------------------------------------------------\n",
      "0 | mel_loss_fn         | MelLoss                           | 0     \n",
      "1 | pitch_loss_fn       | PitchLoss                         | 0     \n",
      "2 | duration_loss_fn    | DurationLoss                      | 0     \n",
      "3 | energy_loss_fn      | EnergyLoss                        | 0     \n",
      "4 | aligner             | AlignmentEncoder                  | 1.0 M \n",
      "5 | forward_sum_loss_fn | ForwardSumLoss                    | 0     \n",
      "6 | bin_loss_fn         | BinLoss                           | 0     \n",
      "7 | preprocessor        | AudioToMelSpectrogramPreprocessor | 0     \n",
      "8 | fastpitch           | FastPitchModule                   | 45.8 M\n",
      "--------------------------------------------------------------------------\n",
      "45.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.8 M    Total params\n",
      "183.103   Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/257 [00:00<?, ?it/s][NeMo W 2023-10-05 17:58:15 nemo_logging:349] /home/giacomo/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "Epoch 0:   0%| | 1/257 [05:51<25:01:51, 352.00s/it, v_num=6-56, train_step_timinReducer buckets have been rebuilt in this iteration.\n",
      "Epoch 0: 100%|█| 257/257 [1:13:28<00:00, 17.15s/it, v_num=6-56, train_step_timin\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                  | 1/37 [00:03<01:51,  3.09s/it]\u001b[A\n",
      "Validation DataLoader 0:   5%|█                  | 2/37 [00:06<01:53,  3.26s/it]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▌                 | 3/37 [00:15<03:00,  5.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 4/37 [00:18<02:32,  4.61s/it]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▌                | 5/37 [00:25<02:40,  5.02s/it]\u001b[A\n",
      "Validation DataLoader 0:  16%|███                | 6/37 [00:52<04:29,  8.69s/it]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▌               | 7/37 [00:54<03:51,  7.72s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|████               | 8/37 [00:55<03:21,  6.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▌              | 9/37 [00:58<03:00,  6.46s/it]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▊             | 10/37 [01:01<02:44,  6.10s/it]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 11/37 [01:04<02:31,  5.83s/it]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▊            | 12/37 [01:10<02:26,  5.85s/it]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 13/37 [01:21<02:31,  6.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▊           | 14/37 [01:27<02:23,  6.26s/it]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 15/37 [01:33<02:16,  6.20s/it]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▊          | 16/37 [01:41<02:12,  6.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 17/37 [01:43<02:02,  6.10s/it]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 18/37 [01:47<01:53,  5.98s/it]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 19/37 [01:54<01:48,  6.02s/it]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 20/37 [01:57<01:39,  5.88s/it]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▏       | 21/37 [02:01<01:32,  5.81s/it]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 22/37 [02:05<01:25,  5.70s/it]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████▏      | 23/37 [02:08<01:18,  5.60s/it]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 24/37 [02:13<01:12,  5.57s/it]\u001b[A\n",
      "Validation DataLoader 0:  68%|████████████▏     | 25/37 [03:51<01:50,  9.25s/it]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 26/37 [03:52<01:38,  8.95s/it]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 27/37 [04:02<01:29,  8.97s/it]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▌    | 28/37 [04:03<01:18,  8.71s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 29/37 [04:05<01:07,  8.47s/it]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▌   | 30/37 [04:07<00:57,  8.25s/it]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████   | 31/37 [04:09<00:48,  8.05s/it]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▌  | 32/37 [04:11<00:39,  7.86s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 33/37 [04:13<00:30,  7.68s/it]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 34/37 [04:15<00:22,  7.51s/it]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 35/37 [04:17<00:14,  7.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▌| 36/37 [04:19<00:07,  7.20s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 37/37 [04:19<00:00,  7.02s/it]\u001b[A\n",
      "Epoch 0: 100%|█| 257/257 [1:18:26<00:00, 18.31s/it, v_num=6-56, train_step_timin\u001b[AEpoch 0, global step 257: 'val_loss' reached inf (best inf), saving model to '/home/giacomo/NeMo/resultITA_TTS/FastPitch/2023-10-05_17-46-56/checkpoints/FastPitch--val_loss=nan-epoch=0.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "!(CUDA_VISIBLE_DEVICES=0 python fastpitch.py --config-path . --config-name=fastpitch_align_ITA.yaml \\\n",
    "  sample_rate=16000 \\\n",
    "  train_dataset=/home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/train_manifest_phonemes.json \\\n",
    "  validation_datasets=/home/giacomo/dataset/MAILABS/it_IT/by_book/female/lisa_caputo/val_manifest_phonemes.json \\\n",
    "  sup_data_path=fastpitch_sup_data_folder \\\n",
    "  exp_manager.exp_dir=resultITA_TTS \\\n",
    "  trainer.check_val_every_n_epoch=1 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. We use `CUDA_VISIBLE_DEVICES=0` to limit training to single GPU.\n",
    "2. For debugging you may also add the following flags: `HYDRA_FULL_ERROR=1`, `CUDA_LAUNCH_BLOCKING=1`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating FastPitch + pretrained HiFi-GAN\n",
    "\n",
    "Let's evaluate the quality of the FastPitch model generated so far using a HiFi-GAN model pre-trained on English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from nemo.collections.tts.models import HifiGanModel, FastPitchModel\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"E non le pare che fosse rosso, ad esempio, il lanternone della Virt\\u00f9 pagana?\" # text input to the model\n",
    "test_id = \"mattiapascal_12_pirandello3_f000058\" # identifier for the audio corresponding to the test text\n",
    "data_path = \"/home/giacomo/il_fu_mattia_pascal/wavs/\" # path to dataset folder with wav files from original dataset\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spec_fastpitch_ckpt(spec_gen_model, v_model, test):\n",
    "    with torch.no_grad():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        parsed = spec_gen_model.parse(str_input=test, normalize=True)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed)\n",
    "        print(spectrogram.size())\n",
    "        audio = v_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "    spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "    audio = audio.to('cpu').numpy()[0]\n",
    "    audio = audio / np.abs(audio).max()\n",
    "    return audio, spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hifigan models\n",
    "hfg_ngc = \"tts_en_lj_hifigan_ft_mixerttsx\" # NGC pretrained model name: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_lj_hifigan \n",
    "vocoder_model = HifiGanModel.from_pretrained(hfg_ngc, strict=False).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fastpitch\n",
    "import glob, os\n",
    "fastpitch_model_path = sorted(\n",
    "    glob.glob(\"FastPitch.ckpt\"), \n",
    "    key=os.path.getmtime)[-1] # path_to_fastpitch_nemo_or_ckpt\n",
    "\n",
    "if \".nemo\" in fastpitch_model_path:\n",
    "    spec_gen_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()\n",
    "else:\n",
    "    spec_gen_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model, test)\n",
    "\n",
    "# visualize the spectrogram\n",
    "if spectrogram is not None:\n",
    "    imshow(spectrogram, origin=\"lower\")\n",
    "    plt.show()\n",
    "\n",
    "# audio\n",
    "print(\"original audio\")\n",
    "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=16000))\n",
    "print(\"predicted audio\")\n",
    "ipd.display(ipd.Audio(audio, rate=16000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning HiFi-GAN\n",
    "\n",
    "Improving speech quality by Finetuning HiFi-GAN on synthesized mel-spectrograms from FastPitch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_text = \"E non le pare che fosse rosso, ad esempio, il lanternone della Virt\\u00f9 pagana?\"\n",
    "test_audio_filepath = \"/home/giacomo/il_fu_mattia_pascal/wavs/mattiapascal_12_pirandello3_f000058.wav\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "\n",
    "def load_wav(audio_file):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "    return samples.transpose()\n",
    "\n",
    "def plot_logspec(spec, axis=None):    \n",
    "    librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(spec, ref=np.max),\n",
    "        y_axis='linear', \n",
    "        x_axis=\"time\",\n",
    "        fmin=0, \n",
    "        fmax=8000,\n",
    "        ax=axis\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original mel spectrogram generated from original audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading original melspec\")\n",
    "y, sr = librosa.load(test_audio_filepath)\n",
    "# change n_fft, win_length, hop_length parameters below based on your specific config file\n",
    "spectrogram2 = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, win_length=1024, hop_length=256)\n",
    "spectrogram = spectrogram2[ :80, :]\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plot_logspec(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel spectrogram predicted from FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading fastpitch melspec via generate_spectrogram\")\n",
    "with torch.no_grad():\n",
    "    text = spec_model.parse(test_audio_text, normalize=False)\n",
    "    spectrogram = spec_model.generate_spectrogram(\n",
    "      tokens=text, \n",
    "      speaker=None,\n",
    "    )\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "plot_logspec(spectrogram)\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above predicted spectrogram has the duration lower in frames which is not equal to the ground truth 498 frames. In order to finetune HiFi-GAN we need mel spectrogram predicted from FastPitch with ground truth alignment and duration.\n",
    "\n",
    "### Mel spectrogram predicted from FastPitch with groundtruth alignment and duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading fastpitch melspec via forward method with groundtruth alignment and duration\")\n",
    "with torch.no_grad():\n",
    "    device = spec_model.device\n",
    "    beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "    text = spec_model.parse(test_audio_text, normalize=False)\n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    audio = load_wav(test_audio_filepath)\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "    attn_prior = torch.from_numpy(\n",
    "      beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "    ).unsqueeze(0).to(text.device)\n",
    "    spectrogram = spec_model.forward(\n",
    "      text=text, \n",
    "      input_lens=text_len, \n",
    "      spec=spect, \n",
    "      mel_lens=spect_len, \n",
    "      attn_prior=attn_prior,\n",
    "      speaker=None,\n",
    "    )[0]\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "print(\"spectrogram shape = \", spectrogram.shape)\n",
    "plot_logspec(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finetuning without groundtruth alignment and duration has artifacts from the original audio (noise) that get passed on as input to the vocoder resulting in artifacts in vocoder output in the form of noise.\n",
    "- <b> On the other hand, `Mel spectrogram predicted from FastPitch with groundtruth alignment and duration` gives the best results because it enables HiFi-GAN to learn mel spectrograms generated by FastPitch as well as duration distributions closer to the real world (i.e. ground truth) durations. </b>\n",
    "\n",
    "From implementation perspective - we follow the same process described in [Finetuning FastPitch for a new speaker](FastPitch_Finetuning.ipynb) - i.e. take the latest checkpoint from FastPitch training and predict spectrograms for each of the input records in `train_manifest_text_normed.json`, `test_manifest_text_normed.json` and `val_manifest_text_normed.json`. NeMo provides an efficient script, [scripts/dataset_processing/tts/generate_mels.py](https://raw.githubusercontent.com/nvidia/NeMo/main/scripts/dataset_processing/tts/generate_mels.py), to generate Mel-spectrograms in the directory `NeMoGermanTTS/mels` and also create new JSON manifests with a suffix `_mel` by adding a new key `\"mel_filepath\"`. For example, `train_manifest_text_normed.json` corresponds to `train_manifest_text_normed_mel.json` saved in the same directory. You can run the following CLI to obtain the new JSON manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_mels.py \\\n",
    "    --cpu \\\n",
    "    --input-json-manifests /home/giacomo/il_fu_mattia_pascal/train_manifest.json /home/giacomo/il_fu_mattia_pascal/test_manifest.json /home/giacomo/il_fu_mattia_pascal/val_manifest.json \\\n",
    "    --fastpitch-model-ckpt {fastpitch_model_path} \\\n",
    "    --output-json-manifest-root ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
