{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "zT04eIhCs9TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "condacolab.check()"
      ],
      "metadata": {
        "id": "2mI4m97uyFKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create --name nemo_tn python==3.8"
      ],
      "metadata": {
        "id": "NpZLC1fJuwuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda update -n base -c conda-forge conda"
      ],
      "metadata": {
        "id": "SxNSu7JFum3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda init bash"
      ],
      "metadata": {
        "id": "rJ7tZGjdzrH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate nemo_tn"
      ],
      "metadata": {
        "id": "vTbyr2gkv4MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda package"
      ],
      "metadata": {
        "id": "y0PwigbT31Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install wget text-unidecode matplotlib>=3.3.2"
      ],
      "metadata": {
        "id": "iNa4bkh4k0eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZivXzmq0YYLj"
      },
      "source": [
        "# VITS and NeMo\n",
        "\n",
        "VITS is a neural network that converts text characters into an audio sample. For more details on the model, please refer to Nvidia's [VITS Model Card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_lj_vits), or the original [paper](https://arxiv.org/abs/2106.06103).\n",
        "\n",
        "VITS like most NeMo models are defined as a LightningModule, allowing for easy training via PyTorch Lightning, and parameterized by a configuration, currently defined via a yaml file and loading using Hydra.\n",
        "\n",
        "Let's take a look using NeMo's pretrained model and how to use it to generate spectrograms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ90eCfdrNIf"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now that we looked at the VITS model, let's see how to train a VITS Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rHG-LERrPRY"
      },
      "outputs": [],
      "source": [
        "# NeMo's training scripts are stored inside the examples/ folder. Let's grab the vits.py file\n",
        "# as well as the vits.yaml file\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/vits.py\n",
        "!(mkdir -p conf \\\n",
        "  && cd conf \\\n",
        "  && wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/conf/vits.yaml \\\n",
        "  && cd ..)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnEzODcorugt"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/NVIDIA/NeMo/releases/download/v0.11.0/test_data.tar.gz \\\n",
        "&& mkdir -p tests/data \\\n",
        "&& tar xzf test_data.tar.gz -C tests/data\n",
        "\n",
        "# Just like ASR, the VITS require .json files to define the training and validation data.\n",
        "!cat tests/data/asr/an4_val.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OD1L-qRyxGUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python manifest_vits.py --data-root /content/drive/MyDrive/ --val-size 0.1 --test-size 0.2"
      ],
      "metadata": {
        "id": "RYXNIDutmcq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DVmGBNymJts"
      },
      "outputs": [],
      "source": [
        "CUDA_VISIBLE_DEVICES=0 python vits.py model.sample_rate=16000 train_dataset=/content/drive/MyDrive/datasetTTS/train_manifest_vits.json validation_datasets=/content/drive/MyDrive/datasetTTS/val_manifest_vits.json trainer.strategy='ddp_find_unused_parameters_true' trainer.check_val_every_n_epoch=10 +init_from_plt_ckpt="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9erGDGZJ1H_p"
      },
      "source": [
        "# Training Data\n",
        "\n",
        "In order to train VITS, it is highly recommended to obtain high quality speech data with the following properties:\n",
        "  - Sampling rate of 22050Hz or higher\n",
        "  - Speech should contain a variety of speech phonemes\n",
        "  - Audio split into segments of 1-10 seconds\n",
        "  - Audio segments should not have silence at the beginning and end\n",
        "  - Audio segments should not contain long silences inside\n",
        "\n",
        "After obtaining the speech data and splitting into training, validation, and test sections, it is required to construct .json files to tell NeMo where to find these audio files.\n",
        "\n",
        "The .json files should adhere to the format required by the `nemo.collections.tts.data.dataset.TTSDataset` class. For example, here is a sample .json file\n",
        "\n",
        "```json\n",
        "{\"audio_filepath\": \"/path/to/audio1.wav\", \"text\": \"the transcription\", \"duration\": 0.82}\n",
        "{\"audio_filepath\": \"/path/to/audio2.wav\", \"text\": \"the other transcription\", \"duration\": 2.1}\n",
        "...\n",
        "```\n",
        "Please note that the duration is in seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6MgTDLFmJtu"
      },
      "source": [
        "## Evaluating VITS\n",
        "\n",
        "Let's evaluate the quality of the VITS model.\n",
        "\n",
        "VITS is end-to-end model, so we don't need any additional models to generate audios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5xVcFBImJtu"
      },
      "outputs": [],
      "source": [
        "!wget https://multilangaudiosamples.s3.us-east-2.amazonaws.com/LJ023-0089.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpGG00x3mJtv"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "target_sr = 22050\n",
        "\n",
        "audio_path = \"./LJ023-0089.wav\"\n",
        "text_raw = \"That is not only my accusation.\"\n",
        "\n",
        "\n",
        "audio_data, orig_sr = sf.read(audio_path)\n",
        "if orig_sr != target_sr:\n",
        "    audio_data = librosa.core.resample(audio_data, orig_sr=orig_sr, target_sr=target_sr)\n",
        "\n",
        "# Let's double-check that everything matches up!\n",
        "print(f\"Duration (s): {len(audio_data)/target_sr}\")\n",
        "print(\"Transcript:\", text_raw)\n",
        "ipd.Audio(audio_data, rate=target_sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0SuENjWmJtw"
      },
      "outputs": [],
      "source": [
        "model = VitsModel.restore_from_checkpoint(\"tts_en_lj_vits\").cpu().eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe_WA2qvmJtx"
      },
      "outputs": [],
      "source": [
        "tokens = model.parse(text_raw)\n",
        "audio_pred = model.convert_text_to_waveform(tokens=tokens).cpu().detach().numpy()\n",
        "\n",
        "print(\"predicted audio\")\n",
        "ipd.Audio(audio_pred, rate=target_sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqlk1LzjmJty"
      },
      "outputs": [],
      "source": [
        "audio_to_mel = model.audio_to_melspec_processor\n",
        "\n",
        "\n",
        "len_audio = torch.tensor(len(audio_data)).view(1, -1)\n",
        "\n",
        "spec_pred, _ = audio_to_mel(torch.tensor(audio_pred).view(1, -1), len_audio)\n",
        "spec_orig, _ = audio_to_mel(torch.tensor(audio_data).view(1, -1), len_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA5LO3ftmJtz"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].imshow(spec_orig[0][0].cpu().detach())\n",
        "ax[1].imshow(spec_pred[0][0].cpu().detach())\n",
        "\n",
        "ax[0].set_title('Original spectrogram')\n",
        "ax[1].set_title('Predicted spectrogram')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:pass"
      ],
      "metadata": {
        "id": "IjvJYkLn-Uvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}