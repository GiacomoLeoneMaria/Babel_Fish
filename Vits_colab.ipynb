{"cells":[{"cell_type":"code","source":["!pip install -q condacolab\n","import condacolab\n","condacolab.install()"],"metadata":{"id":"zT04eIhCs9TZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697143684549,"user_tz":-120,"elapsed":924,"user":{"displayName":"Giacomo Cavallini","userId":"08730528570741284855"}},"outputId":"8adfb8ee-e8e5-4f45-80f9-05300bf80700"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mâœ¨ðŸ°âœ¨ Everything looks OK!\n"]}]},{"cell_type":"code","source":["condacolab.check()"],"metadata":{"id":"2mI4m97uyFKs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697143688299,"user_tz":-120,"elapsed":404,"user":{"displayName":"Giacomo Cavallini","userId":"08730528570741284855"}},"outputId":"2cb4b1e3-c335-42fc-8b85-d81a1b4c9b93"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ¨ðŸ°âœ¨ Everything looks OK!\n"]}]},{"cell_type":"code","source":["!conda create --name nemo python==3.10.12"],"metadata":{"id":"NpZLC1fJuwuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!conda update -n base -c conda-forge conda"],"metadata":{"id":"SxNSu7JFum3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!conda init bash"],"metadata":{"id":"rJ7tZGjdzrH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!source activate nemo"],"metadata":{"id":"vTbyr2gkv4MX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get install sox libsndfile1 ffmpeg\n","!pip install wget text-unidecode matplotlib>=3.3.2"],"metadata":{"id":"iNa4bkh4k0eB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["git clone https://github.com/NVIDIA/NeMo-text-processing\n","cd NeMo-text-processing\n","./reinstall.sh dev"],"metadata":{"id":"y0PwigbT31Lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["apt-get update && apt-get install -y libsndfile1 ffmpeg\n","git clone https://github.com/NVIDIA/NeMo\n","cd NeMo\n","./reinstall.sh dev"],"metadata":{"id":"8R_e_MKFs2t3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZivXzmq0YYLj"},"source":["# VITS and NeMo\n","\n","VITS is a neural network that converts text characters into an audio sample. For more details on the model, please refer to Nvidia's [VITS Model Card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_lj_vits), or the original [paper](https://arxiv.org/abs/2106.06103).\n"]},{"cell_type":"markdown","metadata":{"id":"zZ90eCfdrNIf"},"source":["# Training\n","\n","Now that we looked at the VITS model, let's see how to train a VITS Model\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7rHG-LERrPRY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697144809934,"user_tz":-120,"elapsed":1386,"user":{"displayName":"Giacomo Cavallini","userId":"08730528570741284855"}},"outputId":"c98bd4cd-4075-406c-ee9a-23a48e030b7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-12 21:06:50--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/vits.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1216 (1.2K) [text/plain]\n","Saving to: â€˜vits.pyâ€™\n","\n","vits.py             100%[===================>]   1.19K  --.-KB/s    in 0s      \n","\n","2023-10-12 21:06:50 (42.6 MB/s) - â€˜vits.pyâ€™ saved [1216/1216]\n","\n","--2023-10-12 21:06:50--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/conf/vits.yaml\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5599 (5.5K) [text/plain]\n","Saving to: â€˜vits.yamlâ€™\n","\n","vits.yaml           100%[===================>]   5.47K  --.-KB/s    in 0s      \n","\n","2023-10-12 21:06:51 (82.3 MB/s) - â€˜vits.yamlâ€™ saved [5599/5599]\n","\n"]}],"source":["# NeMo's training scripts are stored inside the examples/ folder. Let's grab the vits.py file\n","# as well as the vits.yaml file\n","!wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/vits.py\n","!(mkdir -p conf \\\n","  && cd conf \\\n","  && wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/conf/vits.yaml \\\n","  && cd ..)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OD1L-qRyxGUN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697144841336,"user_tz":-120,"elapsed":27093,"user":{"displayName":"Giacomo Cavallini","userId":"08730528570741284855"}},"outputId":"7443f906-9b7d-4468-c6c7-038ac061f184"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["while True:pass"],"metadata":{"id":"jrmrTaBDHR4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["python manifest_vits.py --data-root /content/drive/MyDrive/ --val-size 0.1 --test-size 0.2"],"metadata":{"id":"RYXNIDutmcq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","source activate nemo\n","\n","python\n","from nemo.collections.tts.models import VitsModel\n","model = VitsModel.load_from_checkpoint(\"/content/drive/MyDrive/TTS_checkpoint_VITS/VITS/2023-10-11_10-13-46/checkpoints/VITS21.ckpt\")\n","model.save_to(save_path=\"mymodel.nemo\")"],"metadata":{"id":"8-Whkn1PJrhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CUDA_VISIBLE_DEVICES=0 python vits.py model.sample_rate=16000 train_dataset=/content/drive/MyDrive/datasetTTS_malavoglia/train_manifest_vits.json validation_datasets=/content/drive/MyDrive/datasetTTS_malavoglia/val_manifest_vits.json trainer.strategy='ddp_find_unused_parameters_true' trainer.check_val_every_n_epoch=5 exp_manager.exp_dir=/content/drive/MyDrive/TTS_checkpoint_VITS/finetune +init_from_plt_ckpt=/content/drive/MyDrive/TTS_checkpoint_VITS/VITS/2023-10-11_10-13-46/checkpoints/VITS21.ckpt"],"metadata":{"id":"2h_YjdjXWpht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CUDA_VISIBLE_DEVICES=0 python vits.py model.sample_rate=16000 train_dataset=/content/drive/MyDrive/datasetTTS_malavoglia/train_manifest_vits.json validation_datasets=/content/drive/MyDrive/datasetTTS_malavoglia/val_manifest_vits.json trainer.strategy='ddp_find_unused_parameters_true' trainer.check_val_every_n_epoch=5 exp_manager.exp_dir=/content/drive/MyDrive/TTS_checkpoint_VITS/finetune +init_from_nemo_model=./mymodel.nemo"],"metadata":{"id":"dbLo8IJkj-jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CUDA_VISIBLE_DEVICES=0 python vits.py model.sample_rate=16000 train_dataset=/content/drive/MyDrive/datasetTTS_malavoglia/train_manifest_vits.json validation_datasets=/content/drive/MyDrive/datasetTTS_malavoglia/val_manifest_vits.json trainer.strategy='ddp_find_unused_parameters_true' trainer.check_val_every_n_epoch=5 exp_manager.exp_dir=/content/drive/MyDrive/TTS_checkpoint_VITS/finetune"],"metadata":{"id":"Ep9gZnmnvQrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9erGDGZJ1H_p"},"source":["# Training Data\n","\n","In order to train VITS, it is highly recommended to obtain high quality speech data with the following properties:\n","  - Sampling rate of 22050Hz or higher\n","  - Speech should contain a variety of speech phonemes\n","  - Audio split into segments of 1-10 seconds\n","  - Audio segments should not have silence at the beginning and end\n","  - Audio segments should not contain long silences inside\n","\n","After obtaining the speech data and splitting into training, validation, and test sections, it is required to construct .json files to tell NeMo where to find these audio files.\n","\n","The .json files should adhere to the format required by the `nemo.collections.tts.data.dataset.TTSDataset` class. For example, here is a sample .json file\n","\n","```json\n","{\"audio_filepath\": \"/path/to/audio1.wav\", \"text\": \"the transcription\", \"duration\": 0.82}\n","{\"audio_filepath\": \"/path/to/audio2.wav\", \"text\": \"the other transcription\", \"duration\": 2.1}\n","...\n","```\n","Please note that the duration is in seconds.\n"]},{"cell_type":"markdown","metadata":{"id":"p6MgTDLFmJtu"},"source":["## Evaluating VITS\n","\n","Let's evaluate the quality of the VITS model.\n","\n","VITS is end-to-end model, so we don't need any additional models to generate audios."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpGG00x3mJtv"},"outputs":[],"source":["from matplotlib.pyplot import imshow\n","from matplotlib import pyplot as plt\n","import IPython.display as ipd\n","import numpy as np\n","import torch\n","import librosa\n","import soundfile as sf\n","\n","target_sr = 16000\n","\n","audio_path = \"/content/drive/MyDrive/datasetTTS_malavoglia/wavs/imalavoglia_00_verga_f000008.wav\"\n","text_raw = \"Il meccanismo delle passioni che la determinano in quelle basse sfere Ã¨ meno complicato\te potrÃ  quindi osservarsi con maggior precisione.\"\n","\n","\n","audio_data, orig_sr = sf.read(audio_path)\n","if orig_sr != target_sr:\n","    audio_data = librosa.core.resample(audio_data, orig_sr=orig_sr, target_sr=target_sr)\n","\n","# Let's double-check that everything matches up!\n","print(f\"Duration (s): {len(audio_data)/target_sr}\")\n","print(\"Transcript:\", text_raw)\n","ipd.Audio(audio_data, rate=target_sr)"]},{"cell_type":"code","source":["from nemo.collections.tts.models import VitsModel"],"metadata":{"id":"2gDKrVt5xT98"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0SuENjWmJtw"},"outputs":[],"source":["model = torch.load('/content/drive/MyDrive/TTS_checkpoint_VITS/VITS/2023-10-11_10-13-46/checkpoints/VITS--loss_gen_all=21.6121-epoch=279.ckpt',map_location ='cpu')\n","#model_ = VitsModel.load_from_checkpoint(\"/content/drive/MyDrive/TTS_checkpoint_VITS/VITS/2023-10-11_10-13-46/checkpoints/VITS--loss_gen_all=21.6121-epoch=279.ckpt\").eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe_WA2qvmJtx"},"outputs":[],"source":["tokens = model.parse(text_raw)\n","audio_pred = model.convert_text_to_waveform(tokens=tokens).cpu().detach().numpy()\n","\n","print(\"predicted audio\")\n","ipd.Audio(audio_pred, rate=target_sr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqlk1LzjmJty"},"outputs":[],"source":["audio_to_mel = model.audio_to_melspec_processor\n","\n","\n","len_audio = torch.tensor(len(audio_data)).view(1, -1)\n","\n","spec_pred, _ = audio_to_mel(torch.tensor(audio_pred).view(1, -1), len_audio)\n","spec_orig, _ = audio_to_mel(torch.tensor(audio_data).view(1, -1), len_audio)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oA5LO3ftmJtz"},"outputs":[],"source":["fig, ax = plt.subplots(1, 2)\n","\n","ax[0].imshow(spec_orig[0][0].cpu().detach())\n","ax[1].imshow(spec_pred[0][0].cpu().detach())\n","\n","ax[0].set_title('Original spectrogram')\n","ax[1].set_title('Predicted spectrogram')\n","fig.show()"]},{"cell_type":"code","source":["while True:pass"],"metadata":{"id":"IjvJYkLn-Uvz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/GiacomoLeoneMaria/NeMo/blob/main/tutorials/tts/Vits_Training.ipynb","timestamp":1696607518155}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}